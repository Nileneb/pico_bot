{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38ec400d-af46-4bf4-9eef-6513ab301f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pypdf in /home/workbench/.local/lib/python3.10/site-packages (5.0.1)\n",
      "Requirement already satisfied: biopython in /home/workbench/.local/lib/python3.10/site-packages (1.84)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /home/workbench/.local/lib/python3.10/site-packages (from pypdf) (4.12.2)\n",
      "Requirement already satisfied: numpy in /home/workbench/.local/lib/python3.10/site-packages (from biopython) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf biopython "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a4f7ab6-b92e-49a8-84c3-9571855ee8f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeitraum: 2024/10/14 bis 2024/10/17\n",
      "Suchbegriff: (preventive health services[MeSH Terms] OR health promotion[MeSH Terms] OR primary prevention[MeSH Terms] OR \"prävention\"[Title/Abstract] OR \"prevention\"[Title/Abstract]) AND (nutrition[MeSH Terms] OR exercise[MeSH Terms] OR vaccination[MeSH Terms] OR mental health[MeSH Terms] OR lifestyle[MeSH Terms])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Search Backend failed: An error occurred while processing request. Status: 500. Source: /api/search/?r= Details: Search is temporarily unavailable. Please try again later. Details: Cannot connect to SOLR",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 87\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuchbegriff: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch_term\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# PubMed nach Artikeln durchsuchen\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m search_results \u001b[38;5;241m=\u001b[39m \u001b[43mresearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_pubmed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_term\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m pubmed_article_ids \u001b[38;5;241m=\u001b[39m search_results\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIdList\u001b[39m\u001b[38;5;124m'\u001b[39m, [])\n\u001b[1;32m     89\u001b[0m total_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(pubmed_article_ids)\n",
      "Cell \u001b[0;32mIn[2], line 26\u001b[0m, in \u001b[0;36mResearch.search_pubmed\u001b[0;34m(self, start_date, end_date, term)\u001b[0m\n\u001b[1;32m     16\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mterm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m AND medline[sb]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     17\u001b[0m handle \u001b[38;5;241m=\u001b[39m Entrez\u001b[38;5;241m.\u001b[39mesearch(\n\u001b[1;32m     18\u001b[0m     db\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpubmed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m     term\u001b[38;5;241m=\u001b[39mquery,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     maxdate\u001b[38;5;241m=\u001b[39mend_date\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m search_results \u001b[38;5;241m=\u001b[39m \u001b[43mEntrez\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m handle\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m search_results\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/Bio/Entrez/__init__.py:529\u001b[0m, in \u001b[0;36mread\u001b[0;34m(source, validate, escape, ignore_errors)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mParser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataHandler\n\u001b[1;32m    528\u001b[0m handler \u001b[38;5;241m=\u001b[39m DataHandler(validate, escape, ignore_errors)\n\u001b[0;32m--> 529\u001b[0m record \u001b[38;5;241m=\u001b[39m \u001b[43mhandler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m record\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/Bio/Entrez/Parser.py:405\u001b[0m, in \u001b[0;36mDataHandler.read\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile should be opened in binary mode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParseFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m expat\u001b[38;5;241m.\u001b[39mExpatError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mStartElementHandler:\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;66;03m# We saw the initial <!xml declaration, so we can be sure that\u001b[39;00m\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;66;03m# we are parsing XML data. Most likely, the XML file is\u001b[39;00m\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;66;03m# corrupted.\u001b[39;00m\n",
      "File \u001b[0;32m../Modules/pyexpat.c:468\u001b[0m, in \u001b[0;36mEndElement\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/Bio/Entrez/Parser.py:818\u001b[0m, in \u001b[0;36mDataHandler.endErrorElementHandler\u001b[0;34m(self, tag)\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_errors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m--> 818\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(data)\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    820\u001b[0m value \u001b[38;5;241m=\u001b[39m ErrorElement(data, tag)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Search Backend failed: An error occurred while processing request. Status: 500. Source: /api/search/?r= Details: Search is temporarily unavailable. Please try again later. Details: Cannot connect to SOLR"
     ]
    }
   ],
   "source": [
    "# Notwendige Module importieren\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from Bio import Entrez\n",
    "\n",
    "# Research-Klasse definieren\n",
    "class Research:\n",
    "    def __init__(self, email: str, max_results: int = 100, save_directory: str = \"./abstracts\"):\n",
    "        self.email = email\n",
    "        self.max_results = max_results\n",
    "        self.save_directory = save_directory\n",
    "        Entrez.email = self.email\n",
    "\n",
    "    def search_pubmed(self, start_date: str, end_date: str, term: str = \"\") -> dict:\n",
    "        query = f'{term} AND medline[sb]'\n",
    "        handle = Entrez.esearch(\n",
    "            db=\"pubmed\",\n",
    "            term=query,\n",
    "            retmax=self.max_results,\n",
    "            usehistory=\"y\",\n",
    "            datetype=\"pdat\",\n",
    "            mindate=start_date,\n",
    "            maxdate=end_date\n",
    "        )\n",
    "        search_results = Entrez.read(handle)\n",
    "        handle.close()\n",
    "        return search_results\n",
    "\n",
    "    def fetch_summaries(self, article_ids: list) -> list:\n",
    "        summaries_handle = Entrez.esummary(\n",
    "            db=\"pubmed\",\n",
    "            id=\",\".join(article_ids),\n",
    "            retmode=\"xml\"\n",
    "        )\n",
    "        summaries = list(Entrez.parse(summaries_handle))\n",
    "        summaries_handle.close()\n",
    "        return summaries\n",
    "\n",
    "    def fetch_abstract(self, article_id: str) -> str:\n",
    "        try:\n",
    "            abstract_handle = Entrez.efetch(\n",
    "                db=\"pubmed\",\n",
    "                id=article_id,\n",
    "                rettype=\"abstract\",\n",
    "                retmode=\"text\"\n",
    "            )\n",
    "            abstract = abstract_handle.read()\n",
    "            abstract_handle.close()\n",
    "            return abstract.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Abrufen des Abstracts für Artikel {article_id}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "# Hauptteil des Skripts\n",
    "if __name__ == \"__main__\":\n",
    "    # E-Mail-Adresse für Entrez festlegen\n",
    "    email = 'bene.linn@yahoo.de'  # Bitte mit Ihrer E-Mail-Adresse ersetzen\n",
    "    max_results = 100  # Anzahl der gewünschten Artikel\n",
    "    save_directory = './abstracts'  # Verzeichnis zum Speichern der Abstracts\n",
    "\n",
    "    # Verzeichnis erstellen, falls es nicht existiert\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    research = Research(email=email, max_results=max_results, save_directory=save_directory)\n",
    "\n",
    "    # Zeitraum festlegen (diese Woche)\n",
    "    today = datetime.today()\n",
    "    start_of_week = today - timedelta(days=today.weekday())  # Montag dieser Woche\n",
    "    start_date = start_of_week.strftime('%Y/%m/%d')\n",
    "    end_date = today.strftime('%Y/%m/%d')\n",
    "\n",
    "    # Suchbegriff festlegen\n",
    "    search_term = (\n",
    "    '(preventive health services[MeSH Terms] OR health promotion[MeSH Terms] OR '\n",
    "    'primary prevention[MeSH Terms] OR \"prävention\"[Title/Abstract] OR \"prevention\"[Title/Abstract]) '\n",
    "    'AND (nutrition[MeSH Terms] OR exercise[MeSH Terms] OR vaccination[MeSH Terms] OR '\n",
    "    'mental health[MeSH Terms] OR lifestyle[MeSH Terms])'\n",
    "     )\n",
    "  # Ersetzen Sie 'prevention' durch Ihr Interessensgebiet\n",
    "\n",
    "    print(f\"Zeitraum: {start_date} bis {end_date}\")\n",
    "    print(f\"Suchbegriff: {search_term}\")\n",
    "\n",
    "    # PubMed nach Artikeln durchsuchen\n",
    "    search_results = research.search_pubmed(start_date, end_date, term=search_term)\n",
    "    pubmed_article_ids = search_results.get('IdList', [])\n",
    "    total_results = len(pubmed_article_ids)\n",
    "\n",
    "    if total_results == 0:\n",
    "        print(\"Keine Ergebnisse für den angegebenen Zeitraum und Suchbegriff gefunden.\")\n",
    "    else:\n",
    "        print(f\"{total_results} Artikel wurden gefunden.\")\n",
    "        # Summaries abrufen\n",
    "        summaries = research.fetch_summaries(pubmed_article_ids)\n",
    "        print(f\"Anzahl der abgerufenen Zusammenfassungen: {len(summaries)}\")\n",
    "\n",
    "        # Durch die Summaries iterieren\n",
    "        for idx, summary in enumerate(summaries):\n",
    "            article_id = summary['Id']\n",
    "            title = summary.get('Title', 'Kein Titel verfügbar')\n",
    "            authors_list = summary.get('AuthorList', [])\n",
    "            authors = ', '.join(authors_list) if authors_list else 'Keine Autoren verfügbar'\n",
    "            pub_date = summary.get('PubDate', 'Kein Veröffentlichungsdatum verfügbar')\n",
    "            print(f\"\\nArtikel {idx + 1}:\")\n",
    "            print(f\"Artikel-ID: {article_id}\")\n",
    "            print(f\"Titel: {title}\")\n",
    "            print(f\"Autoren: {authors}\")\n",
    "            print(f\"Veröffentlichungsdatum: {pub_date}\")\n",
    "\n",
    "            # Abstract abrufen\n",
    "            abstract = research.fetch_abstract(article_id)\n",
    "            if abstract:\n",
    "                print(\"Abstract erfolgreich abgerufen.\")\n",
    "                # Abstract speichern\n",
    "                sanitized_title = \"\".join(c if c.isalnum() else \"_\" for c in title)[:50]\n",
    "                file_name = f\"Artikel_{article_id}_{sanitized_title}.txt\"\n",
    "                abstract_path = os.path.join(save_directory, file_name)\n",
    "                with open(abstract_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(f\"Titel: {title}\\n\")\n",
    "                    f.write(f\"Autoren: {authors}\\n\")\n",
    "                    f.write(f\"Veröffentlichungsdatum: {pub_date}\\n\\n\")\n",
    "                    f.write(f\"Abstract:\\n{abstract}\")\n",
    "                print(f\"Abstract gespeichert unter: {abstract_path}\")\n",
    "            else:\n",
    "                print(\"Kein Abstract verfügbar.\")\n",
    "        print(\"\\nSkript abgeschlossen.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa091cb-7b27-4634-8f8c-a38a7ca9ee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importieren der notwendigen Module\n",
    "from chain_server.configuration import config\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_milvus.vectorstores.milvus import Milvus\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Setup des NVIDIA Embedding Modells\n",
    "embedding_model = NVIDIAEmbeddings(\n",
    "    model=config.embedding_model.name,      # Beispiel: \"embed-qa-4\"\n",
    "    base_url=str(config.embedding_model.url),\n",
    "    api_key=config.nvidia_api_key,\n",
    "    truncate=\"END\"\n",
    ")\n",
    "\n",
    "# Setup der Milvus Vektordatenbank\n",
    "print(config.milvus.collection_name)\n",
    "\n",
    "vector_store = Milvus(\n",
    "    embedding_function=embedding_model,\n",
    "    connection_args={\"uri\": config.milvus.url},  # Beispiel: \"tcp://localhost:19530\"\n",
    "    collection_name=config.milvus.collection_name,\n",
    "    auto_id=True,\n",
    ")\n",
    "\n",
    "# Funktion zum Hochladen von TXT-Dokumenten\n",
    "def upload_document(file_path):\n",
    "    # Lesen der TXT-Datei\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    # Erstellen eines Dokumentobjekts mit 'page' in den Metadaten\n",
    "    document = Document(\n",
    "        page_content=text,\n",
    "        metadata={\n",
    "            \"source\": file_path,\n",
    "            \"page\": 1  # Fügen Sie das Feld 'page' hinzu\n",
    "        }\n",
    "    )\n",
    "    # Aufteilen des Dokuments in kleinere Abschnitte\n",
    "    text_splitter = RecursiveCharacterTextSplitter()\n",
    "    all_splits = text_splitter.split_documents([document])\n",
    "    # Sicherstellen, dass jedes Split die erforderlichen Metadaten enthält\n",
    "    for split in all_splits:\n",
    "        split.metadata[\"source\"] = split.metadata.get(\"source\", file_path)\n",
    "        split.metadata[\"page\"] = split.metadata.get(\"page\", 1)\n",
    "        split.metadata[\"text\"] = split.page_content  # Fügen Sie das Feld 'text' hinzu\n",
    "    # Hinzufügen der Dokumente zum Vektorspeicher\n",
    "    vector_store.add_documents(all_splits)\n",
    "    return f\"uploaded {file_path}\"\n",
    "\n",
    "# Funktion zum Hochladen mehrerer TXT-Dateien\n",
    "def upload_text_files(folder_path, num_files=None):\n",
    "    i = 0\n",
    "    for file_path in glob.glob(f\"{folder_path}/*.txt\"):\n",
    "        print(upload_document(file_path))\n",
    "        i += 1\n",
    "        if num_files and i >= num_files:\n",
    "            break\n",
    "\n",
    "# Hochladen der TXT-Dateien aus dem './abstracts'-Verzeichnis\n",
    "NUM_DOCS_TO_UPLOAD = None  # Setzen Sie eine Zahl, um die Anzahl der Dokumente zu begrenzen\n",
    "upload_text_files(\"./abstracts\", NUM_DOCS_TO_UPLOAD)\n",
    "\n",
    "# Flushen und Laden der Daten in Milvus\n",
    "vector_store.col.flush()\n",
    "vector_store.col.load()\n",
    "vector_store.col.wait_for_loading_complete()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8645f1ea-4b67-4a49-83e3-f215440de21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Durchführung einer Similarity-Suche\n",
    "query = \"Welche neuen Erkenntnisse gibt es zu präventiven Gesundheitsmaßnahmen?\"\n",
    "docs = vector_store.similarity_search(query)\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf7db486-bd92-4616-a671-6f8478ecef84",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'NVIDIAReranker' from 'langchain_nvidia_ai_endpoints' (/home/workbench/.local/lib/python3.10/site-packages/langchain_nvidia_ai_endpoints/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 79\u001b[0m\n\u001b[1;32m     70\u001b[0m ollama_llm \u001b[38;5;241m=\u001b[39m OllamaLLM(\n\u001b[1;32m     71\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_model\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     72\u001b[0m     base_url\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_model\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m     76\u001b[0m )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Initialisieren des Embedding-Modells (NIM)\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_nvidia_ai_endpoints\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NVIDIAEmbeddings, NVIDIAReranker\n\u001b[1;32m     81\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m NVIDIAEmbeddings(\n\u001b[1;32m     82\u001b[0m     model\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding_model\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     83\u001b[0m     base_url\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding_model\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     84\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnvidia_api_key\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m),  \u001b[38;5;66;03m# Stellen Sie sicher, dass der API-Key gesetzt ist\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEND\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Initialisieren des Rerankers (NIM)\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'NVIDIAReranker' from 'langchain_nvidia_ai_endpoints' (/home/workbench/.local/lib/python3.10/site-packages/langchain_nvidia_ai_endpoints/__init__.py)"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "import glob\n",
    "from langchain.vectorstores import Milvus\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Mapping, Any\n",
    "import requests\n",
    "\n",
    "# Schritt 1: Laden der Konfigurationsdatei\n",
    "with open(\"config.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Schritt 2: Definieren der OllamaLLM-Klasse\n",
    "class OllamaLLM(LLM):\n",
    "    model_name: str\n",
    "    base_url: str\n",
    "    temperature: float = 0.7\n",
    "    max_tokens: int = 512\n",
    "    top_p: float = 1.0\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        url = f\"{self.base_url}/api/generate\"\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"options\": {\n",
    "                \"temperature\": self.temperature,\n",
    "                \"max_tokens\": self.max_tokens,\n",
    "                \"top_p\": self.top_p,\n",
    "            },\n",
    "            \"stream\": False,\n",
    "        }\n",
    "\n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Ollama API returned status code {response.status_code}: {response.text}\")\n",
    "\n",
    "        data = response.json()\n",
    "        generated_text = data.get(\"response\", \"\")\n",
    "\n",
    "        if stop:\n",
    "            for stop_token in stop:\n",
    "                if stop_token in generated_text:\n",
    "                    generated_text = generated_text.split(stop_token)[0]\n",
    "                    break\n",
    "\n",
    "        return generated_text.strip()\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "            \"top_p\": self.top_p,\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ollama\"\n",
    "\n",
    "# Schritt 3: Initialisieren der Modelle basierend auf der Konfiguration\n",
    "\n",
    "# Initialisieren des LLM (Ollama)\n",
    "ollama_llm = OllamaLLM(\n",
    "    model_name=config[\"llm_model\"][\"name\"],\n",
    "    base_url=config[\"llm_model\"][\"url\"],\n",
    "    temperature=0.7,\n",
    "    max_tokens=512,\n",
    "    top_p=1.0,\n",
    ")\n",
    "\n",
    "# Initialisieren des Embedding-Modells (NIM)\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings, NVIDIAReranker\n",
    "\n",
    "embedding_model = NVIDIAEmbeddings(\n",
    "    model=config[\"embedding_model\"][\"name\"],\n",
    "    base_url=config[\"embedding_model\"][\"url\"],\n",
    "    api_key=config.get(\"nvidia_api_key\", \"\"),  # Stellen Sie sicher, dass der API-Key gesetzt ist\n",
    "    truncate=\"END\"\n",
    ")\n",
    "\n",
    "# Initialisieren des Rerankers (NIM)\n",
    "reranker = NVIDIAReranker(\n",
    "    model=config[\"reranking_model\"][\"name\"],\n",
    "    base_url=config[\"reranking_model\"][\"url\"],\n",
    "    api_key=config.get(\"nvidia_api_key\", \"\"),  # Stellen Sie sicher, dass der API-Key gesetzt ist\n",
    ")\n",
    "\n",
    "# Initialisieren der Milvus Vektordatenbank\n",
    "vector_store = Milvus(\n",
    "    embedding_function=embedding_model,\n",
    "    connection_args={\"uri\": os.getenv(\"MILVUS_URL\", \"tcp://localhost:19530\")},\n",
    "    collection_name=config.get(\"milvus_collection_name\", \"collection_1\"),\n",
    "    auto_id=True,\n",
    ")\n",
    "\n",
    "# Schritt 4: Funktionen zum Hochladen von Dokumenten beibehalten\n",
    "\n",
    "def upload_document(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    document = Document(\n",
    "        page_content=text,\n",
    "        metadata={\"source\": file_path, \"page\": 1}\n",
    "    )\n",
    "    text_splitter = RecursiveCharacterTextSplitter()\n",
    "    all_splits = text_splitter.split_documents([document])\n",
    "    for split in all_splits:\n",
    "        split.metadata[\"source\"] = split.metadata.get(\"source\", file_path)\n",
    "        split.metadata[\"page\"] = split.metadata.get(\"page\", 1)\n",
    "        split.metadata[\"text\"] = split.page_content\n",
    "    vector_store.add_documents(all_splits)\n",
    "    return f\"uploaded {file_path}\"\n",
    "\n",
    "def upload_text_files(folder_path, num_files=None):\n",
    "    i = 0\n",
    "    for file_path in glob.glob(f\"{folder_path}/*.txt\"):\n",
    "        print(upload_document(file_path))\n",
    "        i += 1\n",
    "        if num_files and i >= num_files:\n",
    "            break\n",
    "\n",
    "# Hochladen der TXT-Dateien aus dem './abstracts'-Verzeichnis\n",
    "NUM_DOCS_TO_UPLOAD = None  # Setzen Sie eine Zahl, um die Anzahl der Dokumente zu begrenzen\n",
    "upload_text_files(\"./abstracts\", NUM_DOCS_TO_UPLOAD)\n",
    "\n",
    "# Flushen und Laden der Daten in Milvus\n",
    "vector_store.col.flush()\n",
    "vector_store.col.load()\n",
    "vector_store.col.wait_for_loading_complete()\n",
    "\n",
    "# Einrichtung des Conversation Buffers\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Anpassung der Reranking-Funktion\n",
    "def custom_reranker(documents, query):\n",
    "    reranked_docs = reranker.rerank(query, documents)\n",
    "    return reranked_docs\n",
    "\n",
    "# Integrieren des Rerankers in den Retriever\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "retriever.add_rerank_function(custom_reranker)\n",
    "\n",
    "# Erstellen der Conversational Retrieval Chain mit OllamaLLM\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=ollama_llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    return_source_documents=True,\n",
    "    get_chat_history=lambda h: h,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Durchführung einer Abfrage\n",
    "query = \"Welche neuen Erkenntnisse gibt es zu präventiven Gesundheitsmaßnahmen?\"\n",
    "\n",
    "# Ausführen der Kette\n",
    "try:\n",
    "    result = qa_chain({\"question\": query})\n",
    "    # Ausgabe des Ergebnisses\n",
    "    print(\"Antwort:\")\n",
    "    print(result[\"answer\"])\n",
    "except Exception as e:\n",
    "    print(f\"Fehler bei der Anfrage: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c8041f-92ff-431d-b308-2dedfd330e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
