{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0694ee22-1cbe-498a-8f94-f6e0943ecfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (6.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.2)\n",
      "Requirement already satisfied: langchain in /home/workbench/.local/lib/python3.10/site-packages (0.3.3)\n",
      "Requirement already satisfied: biopython in /home/workbench/.local/lib/python3.10/site-packages (1.84)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
      "Requirement already satisfied: pymilvus in /home/workbench/.local/lib/python3.10/site-packages (2.4.7)\n",
      "Requirement already satisfied: redis in /home/workbench/.local/lib/python3.10/site-packages (5.1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/workbench/.local/lib/python3.10/site-packages (from langchain) (3.10.10)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /home/workbench/.local/lib/python3.10/site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/workbench/.local/lib/python3.10/site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/workbench/.local/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/workbench/.local/lib/python3.10/site-packages (from langchain) (0.1.134)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/workbench/.local/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/workbench/.local/lib/python3.10/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/workbench/.local/lib/python3.10/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.10 in /home/workbench/.local/lib/python3.10/site-packages (from langchain) (0.3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.11.17)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in /home/workbench/.local/lib/python3.10/site-packages (from pymilvus) (5.28.2)\n",
      "Requirement already satisfied: pandas>=1.2.4 in /home/workbench/.local/lib/python3.10/site-packages (from pymilvus) (2.2.3)\n",
      "Requirement already satisfied: setuptools>69 in /home/workbench/.local/lib/python3.10/site-packages (from pymilvus) (75.1.0)\n",
      "Requirement already satisfied: ujson>=2.0.0 in /home/workbench/.local/lib/python3.10/site-packages (from pymilvus) (5.10.0)\n",
      "Requirement already satisfied: environs<=9.5.0 in /home/workbench/.local/lib/python3.10/site-packages (from pymilvus) (9.5.0)\n",
      "Requirement already satisfied: grpcio>=1.49.1 in /home/workbench/.local/lib/python3.10/site-packages (from pymilvus) (1.66.2)\n",
      "Requirement already satisfied: milvus-lite<2.5.0,>=2.4.0 in /home/workbench/.local/lib/python3.10/site-packages (from pymilvus) (2.4.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/workbench/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/workbench/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/workbench/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/workbench/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/workbench/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.14.0)\n",
      "Requirement already satisfied: python-dotenv in /home/workbench/.local/lib/python3.10/site-packages (from environs<=9.5.0->pymilvus) (1.0.1)\n",
      "Requirement already satisfied: marshmallow>=3.0.0 in /home/workbench/.local/lib/python3.10/site-packages (from environs<=9.5.0->pymilvus) (3.22.0)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (23.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/workbench/.local/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/workbench/.local/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (4.12.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/workbench/.local/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/workbench/.local/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/workbench/.local/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: tqdm in /home/workbench/.local/lib/python3.10/site-packages (from milvus-lite<2.5.0,>=2.4.0->pymilvus) (4.66.5)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/workbench/.local/lib/python3.10/site-packages (from pandas>=1.2.4->pymilvus) (2024.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/workbench/.local/lib/python3.10/site-packages (from pandas>=1.2.4->pymilvus) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (2.8.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/workbench/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/workbench/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/workbench/.local/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.1.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/workbench/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/workbench/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.10->langchain) (2.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/workbench/.local/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyyaml beautifulsoup4 langchain biopython requests pymilvus redis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2b3310a-c021-4a31-bda0-689207d3f555",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PICO-Frage: messages=[SystemMessage(content=\"You're an AI assistant designed to help with health-related literature searches.\\n            Using the PICO framework, assist in extracting key components from the text,\\n            and formulate a suitable search query for databases like PubMed. You are using a\\n            Retrieval-Augmented Generation (RAG) model with NVIDIA NIMs to identify relevant information.\\n            Based on the provided information, guide through the PICO process:\\n            - **Patient/Population/Problem (P)**: Identify who the patient or population is, and what their health problem is.\\n            - **Intervention (I)**: Determine the intervention of interest (e.g., treatment, exposure).\\n            - **Comparison (C)**: Specify any comparison interventions (if applicable).\\n            - **Outcome (O)**: Define the outcomes of interest (e.g., reduction in symptoms, prevention).\\n            Please provide the necessary details or confirm that you would like me to proceed with formulating the PubMed query.\", additional_kwargs={}, response_metadata={}), AIMessage(content='Extracting information as per PICO framework...', additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Empty id list - nothing todo",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 94\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPICO-Frage:\u001b[39m\u001b[38;5;124m\"\u001b[39m, pico_question)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Suche nach Artikeln basierend auf der PICO-Frage\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m article_ids, summaries \u001b[38;5;241m=\u001b[39m \u001b[43mresearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpico_question\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Lade die ersten 10 Summaries in die Datenbank\u001b[39;00m\n\u001b[1;32m     97\u001b[0m summaries_to_upload \u001b[38;5;241m=\u001b[39m [summary\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo title available\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m summary \u001b[38;5;129;01min\u001b[39;00m summaries]\n",
      "Cell \u001b[0;32mIn[2], line 65\u001b[0m, in \u001b[0;36mResearchWithSummaries.search_articles\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     63\u001b[0m search_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_pubmed(query)\n\u001b[1;32m     64\u001b[0m pubmed_article_ids \u001b[38;5;241m=\u001b[39m search_results\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIdList\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[0;32m---> 65\u001b[0m summaries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_summaries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpubmed_article_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pubmed_article_ids, [\n\u001b[1;32m     67\u001b[0m     {\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m\"\u001b[39m: summary\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo title available\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m summary \u001b[38;5;129;01min\u001b[39;00m summaries\n\u001b[1;32m     77\u001b[0m ]\n",
      "File \u001b[0;32m/project/code/../code/chain_server/fetch_data.py:22\u001b[0m, in \u001b[0;36mResearch.fetch_summaries\u001b[0;34m(self, article_ids)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch_summaries\u001b[39m(\u001b[38;5;28mself\u001b[39m, article_ids: \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m     21\u001b[0m     summaries_handle \u001b[38;5;241m=\u001b[39m Entrez\u001b[38;5;241m.\u001b[39mesummary(db\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpubmed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(article_ids), retmode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m     summaries \u001b[38;5;241m=\u001b[39m \u001b[43mEntrez\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummaries_handle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     summaries_handle\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m summaries\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/Bio/Entrez/__init__.py:529\u001b[0m, in \u001b[0;36mread\u001b[0;34m(source, validate, escape, ignore_errors)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mParser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataHandler\n\u001b[1;32m    528\u001b[0m handler \u001b[38;5;241m=\u001b[39m DataHandler(validate, escape, ignore_errors)\n\u001b[0;32m--> 529\u001b[0m record \u001b[38;5;241m=\u001b[39m \u001b[43mhandler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m record\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/Bio/Entrez/Parser.py:405\u001b[0m, in \u001b[0;36mDataHandler.read\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile should be opened in binary mode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParseFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m expat\u001b[38;5;241m.\u001b[39mExpatError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mStartElementHandler:\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;66;03m# We saw the initial <!xml declaration, so we can be sure that\u001b[39;00m\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;66;03m# we are parsing XML data. Most likely, the XML file is\u001b[39;00m\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;66;03m# corrupted.\u001b[39;00m\n",
      "File \u001b[0;32m../Modules/pyexpat.c:468\u001b[0m, in \u001b[0;36mEndElement\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/Bio/Entrez/Parser.py:818\u001b[0m, in \u001b[0;36mDataHandler.endErrorElementHandler\u001b[0;34m(self, tag)\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_errors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m--> 818\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(data)\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    820\u001b[0m value \u001b[38;5;241m=\u001b[39m ErrorElement(data, tag)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Empty id list - nothing todo"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pymilvus import Collection, connections, utility\n",
    "from redis import Redis\n",
    "from langchain_milvus.vectorstores.milvus import Milvus\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings, ChatNVIDIA\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "from langchain_core.documents import Document\n",
    "from operator import itemgetter\n",
    "\n",
    "# Konfigurierbare Variablen\n",
    "EMAIL = \"bene.linn@yahoo.de\"\n",
    "MILVUS_COLLECTION_NAME = \"pubmed_summaries\"\n",
    "SAVE_DIRECTORY = \"./\"\n",
    "API_KEY = os.getenv(\"NVIDIA_API_KEY\")\n",
    "PICO_TEXT_FILE_PATH = \"./input.txt\"\n",
    "\n",
    "# Füge den Pfad zur Datei hinzu\n",
    "script_dir = os.getcwd()\n",
    "sys.path.append(os.path.join(script_dir, \"../code/chain_server\"))\n",
    "\n",
    "from fetch_data import Research  # Verwende die bestehende Research-Klasse\n",
    "from prompts import CONDENSE_QUESTION_TEMPLATE  # Importiere die Chat-Prompts\n",
    "\n",
    "class ResearchWithSummaries(Research):\n",
    "    def __init__(self, email: str, milvus_collection_name: str, max_results: int = 20, save_directory: str = \"./\"):\n",
    "        super().__init__(email, max_results, save_directory)\n",
    "        self.milvus_collection_name = milvus_collection_name\n",
    "        self.embeddings = NVIDIAEmbeddings(\n",
    "            model=\"nvidia/nv-embedqa-e5-v5\",\n",
    "            base_url=\"http://nv-embedqa-e5-v5:8000/v1\"\n",
    "        )\n",
    "        connections.connect(\"default\", host=\"milvus\", port=\"19530\")\n",
    "        self.vector_store = Milvus(\n",
    "            embedding_function=self.embeddings,\n",
    "            connection_args={\"uri\": \"http://milvus:19530\"},\n",
    "            collection_name=milvus_collection_name,\n",
    "            auto_id=True\n",
    "        )\n",
    "\n",
    "    def upload_summaries_to_db(self, summaries: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Splits the summaries into chunks and uploads them to the vector database for analysis.\n",
    "        \"\"\"\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        all_splits = [text_splitter.split_text(summary) for summary in summaries]\n",
    "        all_splits_flat = [item for sublist in all_splits for item in sublist]  # flatten the list\n",
    "        metadatas = [{} for _ in all_splits_flat]\n",
    "        self.vector_store.add_texts(texts=all_splits_flat, metadatas=metadatas)\n",
    "        print(f\"Uploaded {len(summaries)} summaries to vector database.\")\n",
    "\n",
    "    def search_articles(self, query: str) -> tuple:\n",
    "        \"\"\"\n",
    "        Searches PubMed articles based on the given query and returns the article IDs and summaries.\n",
    "        \"\"\"\n",
    "        search_results = self.search_pubmed(query)\n",
    "        pubmed_article_ids = search_results.get(\"IdList\", [])\n",
    "        summaries = self.fetch_summaries(pubmed_article_ids[:10])\n",
    "        return pubmed_article_ids, [\n",
    "            {\n",
    "                \"Title\": summary.get(\"Title\", \"No title available\"),\n",
    "                \"Authors\": summary.get(\"AuthorList\", [\"No authors available\"]),\n",
    "                \"Source\": summary.get(\"Source\", \"No source available\"),\n",
    "                \"PubDate\": summary.get(\"PubDate\", \"No publication date available\"),\n",
    "                \"Abstract\": summary.get(\"Abstract\", \"No abstract available\"),\n",
    "                \"DOI\": summary.get(\"DOI\", \"No DOI available\"),\n",
    "                \"MeSH\": summary.get(\"MeshHeadingList\", [\"No MeSH terms available\"]),\n",
    "            }\n",
    "            for summary in summaries\n",
    "        ]\n",
    "\n",
    "# Initialisiere die Klasse\n",
    "research = ResearchWithSummaries(email=EMAIL, milvus_collection_name=MILVUS_COLLECTION_NAME, save_directory=SAVE_DIRECTORY)\n",
    "\n",
    "# Beispiel einer Textdatei, aus der eine PICO-Frage extrahiert wird\n",
    "def extract_pico_from_text(file_path: str) -> str:\n",
    "    with open(file_path, \"r\") as file:\n",
    "        text = file.read()\n",
    "    condense_question_prompt = PROMPTS[\"health_search\"].with_config(run_name=\"pico_extraction_prompt\")\n",
    "    return condense_question_prompt.invoke({\"question\": text})\n",
    "\n",
    "# Extrahiere PICO-Informationen aus einer Textdatei\n",
    "pico_question = extract_pico_from_text(PICO_TEXT_FILE_PATH)\n",
    "print(\"PICO-Frage:\", pico_question)\n",
    "\n",
    "# Suche nach Artikeln basierend auf der PICO-Frage\n",
    "article_ids, summaries = research.search_articles(pico_question)\n",
    "\n",
    "# Lade die ersten 10 Summaries in die Datenbank\n",
    "summaries_to_upload = [summary.get(\"Title\", \"No title available\") for summary in summaries]\n",
    "research.upload_summaries_to_db(summaries_to_upload)\n",
    "\n",
    "# Initialisiere die Chat-Schnittstelle mit dem LLM\n",
    "llm = ChatNVIDIA(\n",
    "    model=\"meta/llama3-8b-instruct\",\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key=API_KEY\n",
    ")\n",
    "\n",
    "@chain\n",
    "def chat_with_llm(question: str, history: list) -> str:\n",
    "    \"\"\"\n",
    "    Use LLM to answer questions based on chat history and user query.\n",
    "    \"\"\"\n",
    "    condense_question_prompt = PROMPTS[\"condense\"].with_config(run_name=\"condense_question_prompt\")\n",
    "    condensed_chain = condense_question_prompt | llm | StrOutputParser().with_config(run_name=\"condense_question_chain\")\n",
    "    if history:\n",
    "        return condensed_chain.invoke({\"history\": history, \"question\": question})\n",
    "    return question\n",
    "\n",
    "# Beispiel eines Chat-Verlaufs\n",
    "chat_history = []\n",
    "while True:\n",
    "    user_input = input(\"Your question: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "    response = chat_with_llm(user_input, chat_history)\n",
    "    print(\"Assistant:\", response)\n",
    "    chat_history.append({\"user\": user_input, \"assistant\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e37551-089d-4518-871c-c1580b77584e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747c1001-e06d-46db-a112-9bbd731ecae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
